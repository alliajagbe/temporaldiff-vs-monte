{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n",
      "[0. 0.]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index -11 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 36\u001b[0m\n\u001b[0;32m     34\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(num_actions)\n\u001b[0;32m     35\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[39mprint\u001b[39m(q_network[state])\n\u001b[0;32m     37\u001b[0m     action \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(q_network[state])\n\u001b[0;32m     39\u001b[0m \u001b[39m# take action and observe next state and reward\u001b[39;00m\n",
      "\u001b[1;31mIndexError\u001b[0m: index -11 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# environment parameters\n",
    "num_states = 10\n",
    "num_actions = 2\n",
    "\n",
    "# replay buffer parameters\n",
    "buffer_size = 1000\n",
    "buffer_index = 0\n",
    "replay_buffer = np.zeros((buffer_size, 4))  # state, action, reward, next_state\n",
    "\n",
    "# Q-network parameters\n",
    "q_network = np.zeros((num_states, num_actions))\n",
    "\n",
    "# training parameters\n",
    "epsilon = 0.1\n",
    "max_timesteps = 100\n",
    "batch_size = 32\n",
    "learning_rate = 0.1\n",
    "discount_factor = 0.99\n",
    "num_episodes = 1000\n",
    "\n",
    "# training loop\n",
    "for episode in range(num_episodes):\n",
    "    state = 0\n",
    "    goal = num_states - 1\n",
    "    episode_buffer = []\n",
    "    timestep = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done and timestep < max_timesteps:\n",
    "        # choose action based on Q-network and exploration rate\n",
    "        if np.random.random() < epsilon:\n",
    "            action = np.random.randint(num_actions)\n",
    "        else:\n",
    "            print(state, q_network[state])\n",
    "            action = np.argmax(q_network[state])\n",
    "        \n",
    "        # take action and observe next state and reward\n",
    "        if action == 0:\n",
    "            next_state = state - 1\n",
    "            reward = -1\n",
    "        else:\n",
    "            next_state = state + 1\n",
    "            reward = 1\n",
    "        \n",
    "        # add transition to episode buffer and replay buffer\n",
    "        episode_buffer.append((state, action, reward, next_state))\n",
    "        replay_buffer[buffer_index] = (state, action, reward, next_state)\n",
    "        buffer_index = (buffer_index + 1) % buffer_size\n",
    "        \n",
    "        # update Q-network with minibatch of transitions\n",
    "        if buffer_index >= batch_size:\n",
    "            minibatch = replay_buffer[np.random.choice(buffer_size, batch_size)]\n",
    "            for transition in minibatch:\n",
    "                s, a, r, s_prime = transition\n",
    "                target = r + discount_factor * np.max(q_network[s_prime])\n",
    "                q_network[s, a] = (1 - learning_rate) * q_network[s, a] + learning_rate * target\n",
    "        \n",
    "        # update state and timestep\n",
    "        state = next_state\n",
    "        timestep += 1\n",
    "        \n",
    "        # check if episode is done\n",
    "        if state == goal or timestep == max_timesteps:\n",
    "            done = True\n",
    "            \n",
    "            # update replay buffer with hindsight replay\n",
    "            for i in range(len(episode_buffer)):\n",
    "                s, a, r, s_prime = episode_buffer[i]\n",
    "                new_reward = -1 if i < len(episode_buffer) - 1 else reward\n",
    "                new_transition = (s, a, new_reward, s_prime)\n",
    "                replay_buffer[buffer_index] = new_transition\n",
    "                buffer_index = (buffer_index + 1) % buffer_size"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
